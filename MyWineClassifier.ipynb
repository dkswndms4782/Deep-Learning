{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MyWineClassifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNgTI/IFsRxm8EbxCsljO7b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkswndms4782/Deep-Learning/blob/main/MyWineClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d2JK38XJwDy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "9ca78004-6979-4a6f-d29a-e2cbbb08d19d"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets,layers,models\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import Input, Dense\n",
        "from keras.layers import *\n",
        "from keras.models import Sequential\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras import optimizers\n",
        "drive.mount('/gdrive')\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "white_wine = pd.read_csv(\"./gdrive/My Drive/winequality-white.csv\",  delimiter=\",\")\n",
        "red_wine = pd.read_csv('./gdrive/My Drive/winequality-red.csv',  delimiter=\",\")\n",
        "\n",
        "display(white_wine)\n",
        "display(red_wine)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8fd0506edde4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mwhite_wine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./gdrive/My Drive/winequality-white.csv\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mred_wine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./gdrive/My Drive/winequality-red.csv'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ./gdrive/My Drive/winequality-white.csv does not exist: './gdrive/My Drive/winequality-white.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruNKkLH7VJxC"
      },
      "source": [
        "##model.fit(train_images,train_labels,epochs=5,batch_size=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9F7gqqdUgQA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WNM4-68VXu7"
      },
      "source": [
        "#test_loss,test_acc = model.evaluate(test_images,test_labels,verbose=2)\n",
        "#print(test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDzwcYogTx_J"
      },
      "source": [
        "def generate_data(df, t_r):\n",
        "    df = df.values\n",
        "    X_train,X_test = train_test_split(df,\n",
        "                                      train_size = t_r,\n",
        "                                      shuffle = False,\n",
        "                                      random_state = 1004)\n",
        "    Y_train = X_train[:,11]\n",
        "    X_train = X_train[:,:11]\n",
        "    Y_test = X_test[:,11]\n",
        "    X_test = X_test[:,:11]\n",
        "    Y_train = np_utils.to_categorical(Y_train,11)\n",
        "    Y_test = np_utils.to_categorical(Y_test,11)\n",
        "    return X_train, Y_train, X_test, Y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxlAK3e7UGDb"
      },
      "source": [
        "x_train, y_train, x_test, y_test = generate_data(white_wine, 0.7)\n",
        "X_train, Y_train, X_test, Y_test = generate_data(red_wine,0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zH15XkedtJA"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(units = 32,input_dim = 11,activation = 'relu'))\n",
        "model.add(layers.Dense(units = 11,activation = 'softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "hist = model.fit(x_train,y_train,epochs=200,batch_size=1000,validation_data = (x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kRx4hOKv8wC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS5E0u2rWgBu"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='accuracy')\n",
        "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val_accuracy')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV4888vUwYhU"
      },
      "source": [
        "from keras.layers import Dropout\n",
        "model1 = models.Sequential()\n",
        "model1.add(layers.Dense(units = 1024,input_dim = 11,activation = 'relu'))\n",
        "model1.add(layers.Dense(units = 11,activation = 'softmax'))\n",
        "\n",
        "model1.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "hist1 = model1.fit(x_train,y_train,epochs=200,batch_size=1000,validation_data = (x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm9LwmXZ8ExY"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist1.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist1.history['val_loss'], 'r', label='val loss')\n",
        "\n",
        "acc_ax.plot(hist1.history['accuracy'], 'b', label='accuracy')\n",
        "acc_ax.plot(hist1.history['val_accuracy'], 'g', label='val_accuracy')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6pC-8zpDH_Z"
      },
      "source": [
        "from keras.layers import BatchNormalization\n",
        "model2 = models.Sequential()\n",
        "model2.add(layers.Dense(units = 32,input_dim = 11,activation = 'relu'))\n",
        "model2.add(layers.Dense(units = 11,activation = 'relu'))\n",
        "model2.add(layers.Dense(units = 11,activation = 'relu'))\n",
        "model2.add(layers.Dense(units = 11,activation = 'relu'))\n",
        "model2.add(layers.Dense(units = 11,activation = 'relu'))\n",
        "model2.add(layers.Dense(11,activation = 'softmax'))\n",
        "\n",
        "model2.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model2.summary()\n",
        "\n",
        "hist2 = model2.fit(x_train,y_train,epochs=200,batch_size=1000,validation_data = (x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNn_BmWuZNZm"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist2.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist2.history['val_loss'], 'r', label='val loss')\n",
        "\n",
        "acc_ax.plot(hist2.history['accuracy'], 'b', label='accuracy')\n",
        "acc_ax.plot(hist2.history['val_accuracy'], 'g', label='val_accuracy')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vc4o3BO8ndL"
      },
      "source": [
        "from keras.layers import Dropout\n",
        "model3 = models.Sequential()\n",
        "model3.add(layers.Dense(units = 32,input_dim = 11,activation = 'relu'))\n",
        "model3.add(layers.Dropout(0.2))\n",
        "model3.add(layers.Dense(units = 11,activation = 'softmax'))\n",
        "\n",
        "model3.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "hist3 = model3.fit(x_train,y_train,epochs=200,batch_size=1000,validation_data = (x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-NQ7vY389SQ"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist3.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist3.history['val_loss'], 'r', label='val loss')\n",
        "\n",
        "acc_ax.plot(hist3.history['accuracy'], 'b', label='accuracy')\n",
        "acc_ax.plot(hist3.history['val_accuracy'], 'g', label='val_accuracy')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wezRAq_zDwi3"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(test_acc)\n",
        "\n",
        "test_loss, test_acc = model1.evaluate(x_test, y_test, verbose=2)\n",
        "print(test_acc)\n",
        "\n",
        "test_loss, test_acc = model2.evaluate(x_test, y_test, verbose=2)\n",
        "print(test_acc)\n",
        "\n",
        "test_loss, test_acc = model3.evaluate(x_test, y_test, verbose=2)\n",
        "print(test_acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSOV5XqoKdHw"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "model_ = models.Sequential()\n",
        "model_.add(layers.Dense(units = 32,input_dim = 11,activation = 'relu'))\n",
        "model_.add(layers.Dense(units = 11,activation = 'softmax'))\n",
        "\n",
        "model_.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#4\n",
        "hist_ = model_.fit(X_train,Y_train,epochs=200,batch_size=1000,validation_data = (X_test,Y_test))\n",
        "\n",
        "model1_ = models.Sequential()\n",
        "model1_.add(layers.Dense(units = 1024,input_dim = 11,activation = 'relu'))\n",
        "model1_.add(layers.Dense(units = 11,activation = 'softmax'))\n",
        "\n",
        "model1_.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "hist1_ = model1_.fit(X_train,Y_train,epochs=200,batch_size=1000,validation_data = (X_test,Y_test))\n",
        "\n",
        "from keras.layers import BatchNormalization\n",
        "model2_ = models.Sequential()\n",
        "model2_.add(layers.Dense(units = 32,input_dim = 11,activation = 'relu'))\n",
        "model2_.add(layers.Dense(units = 11,activation = 'relu'))\n",
        "model2_.add(layers.Dense(units = 11,activation = 'relu'))\n",
        "model2_.add(layers.Dense(units = 11,activation = 'relu'))\n",
        "model2_.add(layers.Dense(units = 11,activation = 'relu'))\n",
        "model2_.add(layers.Dense(11,activation = 'softmax'))\n",
        "\n",
        "model2_.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model2_.summary()\n",
        "\n",
        "hist2_ = model2_.fit(X_train,Y_train,epochs=200,batch_size=1000,validation_data = (X_test,Y_test))\n",
        "\n",
        "from keras.layers import Dropout\n",
        "model3_ = models.Sequential()\n",
        "model3_.add(layers.Dense(units = 32,input_dim = 11,activation = 'relu'))\n",
        "model3_.add(layers.Dropout(0.2))\n",
        "model3_.add(layers.Dense(units = 11,activation = 'softmax'))\n",
        "\n",
        "model3_.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "hist3_ = model3_.fit(X_train,Y_train,epochs=200,batch_size=1000,validation_data = (X_test,Y_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UJ1bFqYWNml"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "loss_ax.plot(hist_.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist_.history['val_loss'], 'r', label='val loss')\n",
        "acc_ax.plot(hist_.history['accuracy'], 'b', label='accuracy')\n",
        "acc_ax.plot(hist_.history['val_accuracy'], 'g', label='val_accuracy')\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "plt.show()\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "loss_ax.plot(hist1_.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist1_.history['val_loss'], 'r', label='val loss')\n",
        "acc_ax.plot(hist1_.history['accuracy'], 'b', label='accuracy')\n",
        "acc_ax.plot(hist1_.history['val_accuracy'], 'g', label='val_accuracy')\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "plt.show()\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "loss_ax.plot(hist2_.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist2_.history['val_loss'], 'r', label='val loss')\n",
        "acc_ax.plot(hist2_.history['accuracy'], 'b', label='accuracy')\n",
        "acc_ax.plot(hist2_.history['val_accuracy'], 'g', label='val_accuracy')\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "plt.show()\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "loss_ax.plot(hist3_.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist3_.history['val_loss'], 'r', label='val loss')\n",
        "acc_ax.plot(hist3_.history['accuracy'], 'b', label='accuracy')\n",
        "acc_ax.plot(hist3_.history['val_accuracy'], 'g', label='val_accuracy')\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM64C7MvKdjq"
      },
      "source": [
        "test_loss, test_acc = model_.evaluate(X_test, Y_test, verbose=2)\n",
        "print(test_acc)\n",
        "\n",
        "test_loss, test_acc = model1_.evaluate(X_test, Y_test, verbose=2)\n",
        "print(test_acc)\n",
        "\n",
        "test_loss, test_acc = model2_.evaluate(X_test, Y_test, verbose=2)\n",
        "print(test_acc)\n",
        "\n",
        "test_loss, test_acc = model3_.evaluate(X_test, Y_test, verbose=2)\n",
        "print(test_acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2anqUsztFPbu"
      },
      "source": [
        "wine = pd.concat([red_wine,white_wine],ignore_index = True)\n",
        "X_train, Y_train, X_test, Y_test = generate_data(wine, 0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xraDpM_n-oQ-"
      },
      "source": [
        "model_ = models.Sequential()\n",
        "model_.add(layers.Dense(units = 32,input_dim = 11,activation = 'relu'))\n",
        "model_.add(layers.Dense(units = 11,activation = 'softmax'))\n",
        "\n",
        "model_.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "hist_ = model_.fit(X_train,Y_train,epochs=1000,batch_size=1000,validation_data = (X_test,Y_test))\n",
        "\n",
        "model1_ = models.Sequential()\n",
        "model1_.add(layers.Dense(units = 1024,input_dim = 11,activation = 'relu'))\n",
        "model1_.add(layers.Dense(units = 11,activation = 'softmax'))\n",
        "\n",
        "model1_.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "hist1_ = model1_.fit(X_train,Y_train,epochs=1000,batch_size=1000,validation_data = (X_test,Y_test))\n",
        "\n",
        "from keras.layers import BatchNormalization\n",
        "model2_ = models.Sequential()\n",
        "model2_.add(layers.Dense(units = 32,input_dim = 11,activation = 'relu'))\n",
        "model2_.add(layers.Dense(units = 11,activation = 'relu'))\n",
        "model2_.add(layers.Dense(units = 11,activation = 'relu'))\n",
        "model2_.add(layers.Dense(units = 11,activation = 'relu'))\n",
        "model2_.add(layers.Dense(units = 11,activation = 'relu'))\n",
        "model2_.add(layers.Dense(11,activation = 'softmax'))\n",
        "\n",
        "model2_.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model2_.summary()\n",
        "\n",
        "hist2_ = model2_.fit(X_train,Y_train,epochs=1000,batch_size=1000,validation_data = (X_test,Y_test))\n",
        "\n",
        "from keras.layers import Dropout\n",
        "model3_ = models.Sequential()\n",
        "model3_.add(layers.Dense(units = 32,input_dim = 11,activation = 'relu'))\n",
        "model3_.add(layers.Dropout(0.2))\n",
        "model3_.add(layers.Dense(units = 11,activation = 'softmax'))\n",
        "\n",
        "model3_.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "hist3_ = model3_.fit(X_train,Y_train,epochs=1000,batch_size=1000,validation_data = (X_test,Y_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXYNrQAIJRxq"
      },
      "source": [
        "from keras.layers import BatchNormalization\n",
        "model4_ = models.Sequential()\n",
        "model4_.add(layers.Dense(units = 32,input_dim = 11,activation = 'relu'))\n",
        "model4_.add(layers.Dropout(0.2))\n",
        "model4_.add(layers.Dense(units = 11,activation = 'relu'))\n",
        "model4_.add(layers.Dense(units = 11,activation = 'relu'))\n",
        "model4_.add(layers.Dense(units = 11,activation = 'relu'))\n",
        "model4_.add(layers.Dense(units = 11,activation = 'relu'))\n",
        "model4_.add(layers.Dense(11,activation = 'softmax'))\n",
        "\n",
        "model4_.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model4_.summary()\n",
        "\n",
        "hist4_ = model4_.fit(X_train,Y_train,epochs=1000,batch_size=1000,validation_data = (X_test,Y_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzGQU4-7SwZ8"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgLMjPh_QQbD"
      },
      "source": [
        "test_loss, test_acc = model_.evaluate(X_test, Y_test, verbose=2)\n",
        "print(test_acc)\n",
        "\n",
        "test_loss, test_acc = model1_.evaluate(X_test, Y_test, verbose=2)\n",
        "print(test_acc)\n",
        "\n",
        "test_loss, test_acc = model2_.evaluate(X_test, Y_test, verbose=2)\n",
        "print(test_acc)\n",
        "\n",
        "test_loss, test_acc = model3_.evaluate(X_test, Y_test, verbose=2)\n",
        "print(test_acc)\n",
        "\n",
        "test_loss, test_acc = model4_.evaluate(X_test, Y_test, verbose=2)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uno3JM7MQu5h"
      },
      "source": [
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "loss_ax.plot(hist_.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist_.history['val_loss'], 'r', label='val loss')\n",
        "acc_ax.plot(hist_.history['accuracy'], 'b', label='accuracy')\n",
        "acc_ax.plot(hist_.history['val_accuracy'], 'g', label='val_accuracy')\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "plt.show()\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "loss_ax.plot(hist1_.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist1_.history['val_loss'], 'r', label='val loss')\n",
        "acc_ax.plot(hist1_.history['accuracy'], 'b', label='accuracy')\n",
        "acc_ax.plot(hist1_.history['val_accuracy'], 'g', label='val_accuracy')\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "plt.show()\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "loss_ax.plot(hist2_.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist2_.history['val_loss'], 'r', label='val loss')\n",
        "acc_ax.plot(hist2_.history['accuracy'], 'b', label='accuracy')\n",
        "acc_ax.plot(hist2_.history['val_accuracy'], 'g', label='val_accuracy')\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "plt.show()\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "loss_ax.plot(hist3_.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist3_.history['val_loss'], 'r', label='val loss')\n",
        "acc_ax.plot(hist3_.history['accuracy'], 'b', label='accuracy')\n",
        "acc_ax.plot(hist3_.history['val_accuracy'], 'g', label='val_accuracy')\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "plt.show()\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "loss_ax.plot(hist4_.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist4_.history['val_loss'], 'r', label='val loss')\n",
        "acc_ax.plot(hist4_.history['accuracy'], 'b', label='accuracy')\n",
        "acc_ax.plot(hist4_.history['val_accuracy'], 'g', label='val_accuracy')\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iassyf-1nhfE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}